# Stubs for pyspark.ml.base (Python 3.5)
#

from typing import overload
from typing import Any, Callable, Dict, Generic, Iterable, List, Optional, TypeVar
from pyspark.ml.param import Params, Param
from pyspark.ml.param.shared import HasInputCol, HasOutputCol
from pyspark.sql.column import Column
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.types import DataType, StructType

M = TypeVar("M", bound=Transformer)
ParamMap = Dict[Param, Any]

class _FitMultipleIterator:
    fitSingleModel: Any = ...
    numModel: Any = ...
    counter: int = ...
    lock: Any = ...
    def __init__(self, fitSingleModel: Any, numModels: Any) -> None: ...
    def __iter__(self): ...
    def __next__(self): ...
    def next(self): ...

class Estimator(Params, Generic[M]):
    __metaclass__ = ...  # type: Any
    @overload
    def fit(self, dataset: DataFrame, params: Optional[ParamMap] = ...) -> M: ...
    @overload
    def fit(self, dataset: DataFrame, params: List[ParamMap]) -> List[M]: ...
    def fitMultiple(self, dataset: DataFrame, params: List[ParamMap]) -> Iterable[M]: ...

class Transformer(Params):
    __metaclass__ = ...  # type: Any
    def transform(self, dataset: DataFrame, params: Optional[ParamMap] = ...) -> DataFrame: ...

class Model(Transformer):
    __metaclass__ = ...  # type: Any

class UnaryTransformer(HasInputCol, HasOutputCol, Transformer):
    def createTransformFunc(self) -> Callable: ...
    def outputDataType(self) -> DataType: ...
    def validateInputType(self, inputType: DataType) -> None: ...
    def transformSchema(self, schema: StructType) -> StructType: ...
