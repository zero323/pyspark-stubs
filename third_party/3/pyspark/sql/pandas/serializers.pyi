from pyspark.serializers import (
    Serializer as Serializer,
    UTF8Deserializer as UTF8Deserializer,
    read_int as read_int,
    write_int as write_int,
)
from typing import Any

basestring = str
unicode = str
xrange = range

class SpecialLengths:
    END_OF_DATA_SECTION: int = ...
    PYTHON_EXCEPTION_THROWN: int = ...
    TIMING_DATA: int = ...
    END_OF_STREAM: int = ...
    NULL: int = ...
    START_ARROW_STREAM: int = ...

class ArrowCollectSerializer(Serializer):
    serializer: Any = ...
    def __init__(self) -> None: ...
    def dump_stream(self, iterator: Any, stream: Any): ...
    def load_stream(self, stream: Any) -> None: ...

class ArrowStreamSerializer(Serializer):
    def dump_stream(self, iterator: Any, stream: Any) -> None: ...
    def load_stream(self, stream: Any) -> None: ...

class ArrowStreamPandasSerializer(ArrowStreamSerializer):
    def __init__(
        self, timezone: Any, safecheck: Any, assign_cols_by_name: Any
    ) -> None: ...
    def arrow_to_pandas(self, arrow_column: Any): ...
    def dump_stream(self, iterator: Any, stream: Any) -> None: ...
    def load_stream(self, stream: Any) -> None: ...

class ArrowStreamPandasUDFSerializer(ArrowStreamPandasSerializer):
    def __init__(
        self,
        timezone: Any,
        safecheck: Any,
        assign_cols_by_name: Any,
        df_for_struct: bool = ...,
    ) -> None: ...
    def arrow_to_pandas(self, arrow_column: Any): ...
    def dump_stream(self, iterator: Any, stream: Any): ...

class CogroupUDFSerializer(ArrowStreamPandasUDFSerializer):
    def load_stream(self, stream: Any) -> None: ...
