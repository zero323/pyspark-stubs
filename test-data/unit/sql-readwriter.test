[case readWriterOptions]
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

spark.read.option("foo", 1)
spark.createDataFrame([(1, 2)], ["foo", "bar"]).write.option("bar", True)

from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

spark.read.option("foo", True).option("foo", 1).option("foo", 1.0).option("foo", "1").option("foo", None)
spark.readStream.option("foo", True).option("foo", 1).option("foo", 1.0).option("foo", "1").option("foo", None)

spark.read.options(foo=True, bar=1).options(foo=1.0, bar="1", baz=None)
spark.readStream.options(foo=True, bar=1).options(foo=1.0, bar="1", baz=None)

spark.read.load(foo=True)
spark.readStream.load(foo=True)

spark.read.load(foo=["a"])  # E: Argument "foo" to "load" of "DataFrameReader" has incompatible type "List[str]"; expected "Union[bool, float, int, str, None]"
spark.read.option("foo", (1, ))  # E: Argument 2 to "option" of "DataFrameReader" has incompatible type "Tuple[int]"; expected "Union[bool, float, int, str, None]"
spark.read.options(bar={1})  # E: Argument "bar" to "options" of "DataFrameReader" has incompatible type "Set[int]"; expected "Union[bool, float, int, str, None]"
[out]
