[case scalarUDF]
from pyspark.sql.pandas.functions import pandas_udf, PandasUDFType
import pandas.core.series  # type: ignore[import]
import pandas.core.frame  # type: ignore[import]

@pandas_udf("str", PandasUDFType.SCALAR)
def f(x: pandas.core.series.Series) -> pandas.core.series.Series:
    return x

@pandas_udf("str", PandasUDFType.SCALAR)
def g(x: pandas.core.series.Series, y: pandas.core.series.Series) -> pandas.core.series.Series:
    return x

@pandas_udf("str", PandasUDFType.SCALAR)
def h(*xs: pandas.core.series.Series) -> pandas.core.series.Series:
    return xs[0]

@pandas_udf("str", PandasUDFType.SCALAR)
def k(x: pandas.core.frame.DataFrame, y: pandas.core.series.Series) -> pandas.core.series.Series:
    return x

pandas_udf(lambda x: x, "str", PandasUDFType.SCALAR)
pandas_udf(lambda x, y: x, "str", PandasUDFType.SCALAR)
pandas_udf(lambda *xs: xs[0], "str", PandasUDFType.SCALAR)
[out]

[case scalarIterUDF]
from pyspark.sql.pandas.functions import pandas_udf, PandasUDFType
from pyspark.sql.types import IntegerType
import pandas.core.series  # type: ignore[import]
from typing import Iterable

@pandas_udf(IntegerType(), PandasUDFType.SCALAR_ITER)
def f(xs: pandas.core.series.Series) -> Iterable[pandas.core.series.Series]:
    for x in xs:
        yield x + 1
[out]

[case groupedMapUdf]
from pyspark.sql.session import SparkSession
from pyspark.sql.types import StructField, StructType, LongType
from pyspark.sql.pandas.functions import pandas_udf, PandasUDFType
import pandas.core.frame  # type: ignore[import]

@pandas_udf("id long", PandasUDFType.GROUPED_MAP)
def f(pdf: pandas.core.frame.DataFrame) -> pandas.core.frame.DataFrame:
   return pdf

spark = SparkSession.builder.getOrCreate()

dfg = spark.range(1).groupBy("id")
dfg.apply(f)

@pandas_udf("id long", PandasUDFType.GROUPED_MAP)
def g(key, pdf: pandas.core.frame.DataFrame) -> pandas.core.frame.DataFrame:
   return pdf

dfg.apply(g)


def h(pdf: pandas.core.frame.DataFrame) -> pandas.core.frame.DataFrame:
   return pdf

dfg.applyInPandas(h, "id long")
dfg.applyInPandas(h, StructType([StructField("id", LongType())]))
[out]

[case groupedAggUDF]
# Let's keep this one to make sure compatibility imports work
from pyspark.sql.functions import pandas_udf, PandasUDFType
from pyspark.sql.types import IntegerType
import pandas.core.series  # type: ignore[import]

@pandas_udf(IntegerType(), PandasUDFType.GROUPED_AGG)
def f(x: pandas.core.series.Series) -> int:
    return 42

@pandas_udf("int", PandasUDFType.GROUPED_AGG)
def g(x: pandas.core.series.Series, y: pandas.core.series.Series) -> int:
    return 42

@pandas_udf("int", PandasUDFType.GROUPED_AGG)
def h(*xs: pandas.core.series.Series) -> int:
    return 42

pandas_udf(lambda x: 42, "str", PandasUDFType.GROUPED_AGG)
pandas_udf(lambda x, y: 42, "str", PandasUDFType.GROUPED_AGG)
pandas_udf(lambda *xs: 42, "str", PandasUDFType.GROUPED_AGG)
[out]


[case mapIterUdf]
from pyspark.sql.session import SparkSession
from typing import Iterable
import pandas.core.frame  # type: ignore[import]

spark = SparkSession.builder.getOrCreate()

def f(batch_iter: Iterable[pandas.core.frame.DataFrame]) -> Iterable[pandas.core.frame.DataFrame]:
    for pdf in batch_iter:
        yield pdf[pdf.id == 1]

spark.range(1).mapInPandas(f, "id long").show()
[out]


[case legacyUDF]
from pyspark.sql.functions import udf

udf(lambda x: x, "string")

@udf("string")
def f(x: str) -> str:
    return x
[out]

[case cogroupedAggUdf]
from pyspark.sql.session import SparkSession
import pandas.core.frame  # type: ignore[import]
from  pyspark.sql.types import StructType, StructField, LongType

spark = SparkSession.builder.getOrCreate()

dfg1 = spark.range(1).groupBy("id")
dfg2 = spark.range(1).groupBy("id")

def f(x: pandas.core.frame.DataFrame, y: pandas.core.frame.DataFrame) -> pandas.core.frame.DataFrame:
    return x

dfg1.cogroup(dfg2).applyInPandas(f, "id int")
dfg1.cogroup(dfg2).applyInPandas(f, StructType([StructField("id", LongType())]))
[out]
