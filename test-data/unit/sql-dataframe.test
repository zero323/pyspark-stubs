[case sampling]
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
df = spark.range(1)
df.sample(1.0)
df.sample(0.5, 3)
df.sample(fraction=0.5, seed=3)
df.sample(withReplacement=True, fraction=0.5, seed=3)
df.sample(fraction=1.0)
df.sample(False, fraction=1.0)

# Will raise a runtime error, though not a typecheck error, as bool is
# duck-type compatible with float.
df.sample(True)

df.sample(withReplacement=False) # E: No overload variant of "sample" of "DataFrame" matches argument type "bool" \
                                 # N: Possible overload variants: \
                                 # N:     def sample(self, fraction: float, seed: Optional[int] = ...) -> DataFrame \
                                 # N:     def sample(self, withReplacement: Optional[bool], fraction: float, seed: Optional[int] = ...) -> DataFrame

[out]

[case selectColumns]
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder.getOrCreate()

data = [('Alice', 1)]
df = spark.createDataFrame(data, schema="name str, age int")

df.select(["name", "age"])
df.select([col("name"), col("age")])

df.select(["name", col("age")])  # E: Argument 1 to "select" of "DataFrame" has incompatible type "List[object]"; expected "Union[List[Column], List[str]]"

[out]

[case groupBy]
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder.getOrCreate()

data = [('Alice', 1)]
df = spark.createDataFrame(data, schema="name str, age int")

df.groupBy(["name", "age"])
df.groupby(["name", "age"])
df.groupBy([col("name"), col("age")])
df.groupby([col("name"), col("age")])


df.groupBy(["name", col("age")])  # E: Argument 1 to "groupBy" of "DataFrame" has incompatible type "List[object]"; expected "Union[List[Column], List[str]]"
df.groupby(["name", col("age")])  # E: Argument 1 to "groupby" of "DataFrame" has incompatible type "List[object]"; expected "Union[List[Column], List[str]]"

[out]

[case rollup]
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder.getOrCreate()

data = [('Alice', 1)]
df = spark.createDataFrame(data, schema="name str, age int")

df.rollup(["name", "age"])
df.rollup([col("name"), col("age")])


df.rollup(["name", col("age")])  # E: Argument 1 to "rollup" of "DataFrame" has incompatible type "List[object]"; expected "Union[List[Column], List[str]]"

[out]

[case cube]
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder.getOrCreate()

data = [('Alice', 1)]
df = spark.createDataFrame(data, schema="name str, age int")

df.cube(["name", "age"])
df.cube([col("name"), col("age")])


df.cube(["name", col("age")])  # E: Argument 1 to "cube" of "DataFrame" has incompatible type "List[object]"; expected "Union[List[Column], List[str]]"

[out]



[case dropColumns]
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit, col
spark = SparkSession.builder.getOrCreate()
df = spark.range(1)
df.drop("id")
df.drop("id", "foo")
df.drop(df.id)

df.drop(col("id"), col("foo"))  # E: No overload variant of "drop" of "DataFrame" matches argument types "Column", "Column" \
                                # N: Possible overload variant:     \
                                # N:     def drop(self, *cols: str) -> DataFrame \
                                # N:     <1 more non-matching overload not shown>

[out]

[case fillNullValues]
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
df = spark.createDataFrame([(1,2)], schema=("id1", "id2"))

df.fillna(value=1, subset="id1")
df.fillna(value=1, subset=("id1", "id2"))
df.fillna(value=1, subset=["id1"])

[out]
